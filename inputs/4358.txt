While developing the Syuzhet package–a toolÂ forÂ tracking relative shifts in narrative sentiment–I spent a fair amountÂ of time gut-checking whether the sentiment values returned by the machine methods were a good match for my ownÂ sense of the narrative sentiment. Â Between 70% andÂ 80% of the time, they were what I considered to be good sentence level matches. . . but sentences were not my primary unit of interest. Rather, IÂ wanted a way to assess whether the story shapesÂ that the tool produced by tracking changes in sentiment were a good approximation of central shifts in the “emotional trajectory” of a narrative. Â This emotional trajectory was something thatÂ Kurt Vonnegut had described in aÂ lectureÂ about the simple shapes of stories. Â On a chalkboard, Vonnegut graphed stories ofÂ good fortune and ill fortune in a demonstration that he calls “an exercise in relativity.” Â He was not interested inÂ the precise high and lows in a given book, but instead with the highs and lows of the bookÂ relative to each other. Blood Meridian and The Devil Wears PradaÂ are two very different books. The former is way, way more negative. Â WhatÂ Vonnegut was interested in understanding was not whether McCarthy’sÂ book was more wholly negativeÂ than Weisberger’s, he was interested in understanding the internal dynamics of shifting sentiment: where in aÂ book would we find the lowest low relative to the highest high. Implied in Vonnegut’s lecture wasÂ the idea thatÂ this tracking of relative high and lows couldÂ serve asÂ a proxy for something like “plot structure” or “syuzhet.” This was an interesting idea, and sentiment analysis offered a possible way forward. Â Unfortunately, the best work in sentiment analysis has been in very different domains. Â Could sentiment analysis tools and dictionaries that were designed to assess sentiment in movie reviews also detect subtle shifts in the language of prose fiction? Could these methods handle irony, metaphor, and so forth? Â Some people, especially if they looked only at the results of a few sentences, might reject the whole idea out of hand. Movie reviews and fiction, hogwash! Â Instead of rejecting the idea, I sat down andÂ human coded the sentiment of every sentence in Joyce’s Portrait of the Artist. I then developed Syuzhet so that I could apply and compare four different sentiment detection techniques to my own human codings. This human coding business is nuanced. Â Some sentences are tricky. Â But it’s not the sarcasmÂ or the irony or the metaphor that is tricky. The really hard sentences are the ones that are equal parts positive and negative sentiment. Consider this contrived example: “I hated the way he looked at me that morning, andÂ I was gladÂ that he had become my friend.” Is that a positive or negative sentence? Â GivenÂ the coordinating “and” perhaps the second half is more important than the first part? Â I coded sentences such asÂ this as neutral, and thankfully these were the outliers and not the norm. Most of the time–even in a complex novel like Portrait where the style and complexity of the sentences are bothÂ evolving with the maturationÂ of the protagonist–it was fairly easy to make a determination of positive, negative, or neutral. It turns out that when you do this sort of close reading you learn a lot about the way that authors write/express/manipulate “sentiment.” Â One thing I learnedÂ was that tricky sentences, such as the one above, are usually surrounded by other sentences that are less tricky. Â In fact, in manyÂ random passages thatÂ I examined from other books, and in the entirety ofÂ Portrait, tricky sentences wereÂ usually followed or preceded by other simple sentences that would clarify the sentiment of the larger passage. Â This is an important observation because at the level of an individual sentence, we know that the various machine methods are not super effective.[1] Â That said, I was pretty surprised by the amount of sentence level agreement in my ad hoc test. Â On a sentence by sentence basis, here is how the four methods in the packageÂ performed:[2] Bing 84% agreement Afinn 80% agreement Stanford 60% agreement NRC 50% agreement These results surprised me. Â I was shocked that the more awesomeÂ Stanford method did not outperform the others. I was so shocked, in fact, that I figured I must have done something wrong. Â The Stanford sentiment tagger, for example, thinks that the followingÂ sentence from Joyces Portrait is negative. “Once upon a time and a very good time it was there was a moocow coming down along the road and this moocow that was coming down along the road met a nicens little boy named baby tuckoo.” It was a “very good time.” How could that be negative? Â I think “a very good time” isÂ positive and so do the other methods. The Stanford taggerÂ also indicated that theÂ sentence “He sang that song” is slightly negative. Â All of the other methods scored it as neutral, and so did I. I’m a huge fan of the Stanford tagger;Â I’veÂ been impressed byÂ the way that itÂ handles negation, but perhaps when all is said and done it is simply not well-suited to literary prose where the syntactical constructions can be far more complicated than typical utilitarianÂ prose? I need more time to study how the Stanford taggerÂ behaved on this problem, soÂ I’m just going to exclude it fromÂ the rest of this report. Â My hypothesis, however, is that it is far more sensitive to register/genre than the dictionary based methods. So, as I was saying,Â whatÂ happens with sentiment in actual prose fiction is usually achieved over a series of sentences. That simile, thatÂ bit ofÂ irony,Â thatÂ negated sentenceÂ is typically followed and/or precededÂ by a series of more direct sentences expressing the sentiment ofÂ the passage. Â For example, “She was not ugly. Â She was exceedinglyÂ beautiful.” “I watched him with disgust. He ateÂ like a pig.” Prose, at least the prose that I studied in this experiment, is rarely composed of sustained irony, sustained negation, sustained metaphor, etc. Â Usually authors provide usÂ withÂ lots of clues about the sentiment we are meant to experience, and over the course of several sentences, a paragraph, or a page, the sentiment tends to become less ambiguous. So instead of just testing the machine methods against myÂ human sentiments on a sentence by sentence basis, I split Joyce’s portrait into 20 equally sized chunks, and calculatedÂ the mean sentiment of each. Â I then compared those meansÂ to the means of my own human coded sentiments. Â These were the results: Bing 80% agreement Afinn 85% agreement NRC 90% agreement Not bad. Â But of course any time weÂ apply a chunking method like this weÂ risk breaking the text right in the middle of a key passage. Â And, as we increase the number of chunks and effectively decrease the size of each passage, the values tend to decrease. I ran the same test using 100Â segmentsÂ and saw this: Bing 73% agreement Afinn 77% agreement NRC 58% agreement (ouch) Figure 1 graphsÂ how the AFinnÂ method (with 77% agreement over 100 segments) tracked the sentiment compared to my human sentiments. Figure 1 NextÂ I transformed all of the sentiment vectors (machine and human) using the get_transformed_valuesÂ function. Â I thenÂ calculatedÂ the amount of agreement. With the low pass filter set to the default of 3, I observed the following agreement: Bing 73% agreement Afinn 74% agreement NRC 86% agreement With the low pass filter set to 5, I observed the following agreement: Bing 87% agreement Afinn 93% agreement NRC 90% agreement Figure 2 graphsÂ how the transformed AFinnÂ method tracked narrative changes in sentiment compared to my human sentiments.[3] Figure 2 As I have said elsewhere, my primary reason for open-sourcing thisÂ code was so that others could plot some narrativesÂ of their own and see if the shapes track well with their humanÂ sense of the emotional trajectories. Â If you do that, and you have successes or failure, I’d be very interested in hearing from you (please send me an email). Given all of the above, I suppose my current working benchmark for human to machine accuracyÂ is something like ~80%. Â Frankly, though, I’m more interested in the big picture and whether or not the overall shapes produced by this method map well onto our human sense of a book’s emotional trajectory. Â They certainly do seem to map well with my sense ofÂ Portrait of the Artist, and with many other books in my library, but what about your favorite novel? FOOTNOTES: [1]Â For what it is worth, the same can probably be said about us, the human beings. Â Given a singleÂ sentence with no context, we could probably argue about itsÂ positive or negativeness. [2] Each method uses a slightly different value range, so when I write of “agreement,” Â I mean only that theÂ machine method agreed with the human (me) that aÂ given sentenceÂ was positively or negatively charged. Â My rating scale consisted of three values: 1, 0, -1 (positive, neutral, negative). I did not test the extent of the positiveness or the degree of negativeness. [3] I explored low-pass values in increments of 5 all the way to 100. Â The percentages of agreement were consistently betweenÂ 70 andÂ 90.
