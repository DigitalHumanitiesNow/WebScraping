Annie Swafford has raised a couple of interesting points about how the syuzhet package works to estimate the emotional trajectory in a novel, a trajectory which I have suggested serves as a handy proxy for plot (in the spirit of Kurt Vonnegut). Annie expresses some concern about the level of precision the tool provides and suggest that dictionary based methods (such as the three I include as options in syuzhet) are not reliable. She writes “Sentiment analysis based solely on word-by-word lexicon lookups is really not state-of-the-art at all.” That’s fair, I suppose, but those three lexicons are benchmarks of some importance, and they deserve to be included in the package if for no other reason than for comparison. Â Frankly, I don’t think anyÂ of the current sentiment detection methods are especiallyÂ reliable. The Stanford tagger has a reputation for beingÂ the main contender forÂ the title of “best in the open sourceÂ market,” butÂ even it hovers aroundÂ 80 – 83% accuracy. Â My own tests have shown that performanceÂ depends a good dealÂ on genre/register. But Annie seems especially concerned about the three dictionary methods in the package. She writes “sentiment analysis as it is implemented in the syuzhet package does not correctly identify the sentiment of sentences.” Given that sentiment is a subtle and nuanced thing, I’m not sure thatÂ “correct” is the right word here. I’m not convinced there is a “correct” answer when it comes to this question of valence. I do agree, however, that some answers are more or less correct than others and that to be useful we need to be on the closer side. The question to address, then, is whether we are close enough, and that’s a hard one. We would probably find a good deal of human agreement when it comes to the extremes of sentiment, but there are a lot of tricky cases, grey areas where I’m not sure we would all agree. Â We certainly cannot expect the tool to perform better than aÂ person, so we need some flexibility in our definition of “correct.” Take, for example, the sentence “I studied at Leland Stanford Junior University.” The state-of-the-art Stanford sentiment parser scores this sentence as “negative.” I think that is incorrectÂ (you are welcome to disagree;-). The “bing” method, that I have implemented as the default in syuzhet, scores this sentence as neutral, as does the “afinn” method (also in the package). The NRC method scores it as slightly positive. So, which one is correct? We could go all Derrida on this sentence and deconstruct each word, unpack what “junior” really means. We could probably even “problematize” it! . . . But let’s not. What Annie writesÂ about dictionary based methods not being the most state-of-the-artÂ is true from a technical standpoint but sophisticated methodsÂ and complexity do not necessarily correlate with results. Â Annie suggest thatÂ “getting the Stanford package to work consistently would go a long way towards addressing some of these issues,” but as we saw with the sentence above, simple beat sophisticated, hands down[1]. Consider anotherÂ sentence: “Syuzhet is not beautiful.” All four methods score this sentence as positive, even the Stanford tool, which tends to doÂ a better job with negation, says “positive.” It is easy toÂ find oppositeÂ cases where sophisticated wins the day. Consider this more complex sentence: “He was not the sort of man that one would describe as especially handsome.” Both NRC and Afinn score this sentence as neutral, Bing scores it slightly positive and Stanford scores it slightly negative. When it comes to negation, the Stanford tool tends to perform a bit better, but not always. The very similar sentence “She was not the sort of woman that one would describe as beautiful” is scored slightly positive by all four methods. What I have found in my testing is thatÂ these four methods usually agree with each other, not exactly but close enough. Because the Stanford parser is very computationally expensive and requires special installation, I focused the examples in the Syuzhet Package Vignette on the three dictionary based methods. All three are lightning fast by comparison, and all three have the benefit of simplicity. But, are they good enough compared to theÂ more sophisticated Stanford parser? BelowÂ are two graphics showing how the methods stack up over a longer piece of text. The first image showsÂ sentiment using percentage based segmentation as implemented in the get_percentage_values() function. Four Methods Compared using Percentage Segmentation The three dictionary methods appear to be a bit closer, but all four methods doÂ create the same basic shape. Â The nextÂ image shows the same data after normalization using the get_transformed_values() function. Â Here the similarity is even more pronounced. Four Methods Compared Using Transformed Values While we could legitimately argue about the accuracy of one sentence here or one sentence there, as Annie has done, that is not the point. The point is to reveal a latent emotional trajectory that represents the general sense of the novel’s plot. In this example, all four methods make it pretty clear what that shape is: This is what Vonnegut called “Man in Hole.” The sentence level precision that Annie wants is probably not possible, at least not right now. Â While I am sympathetic to theÂ position, I would argue that for this particular use case, it really does not matter. Â The tool simply has to be good enough, not perfect. Â If the overall shape mirrors our sense of the novel’s plot, then the tool is working, and this is the area where I think there is stillÂ a lot of validation work to do. Â Part of the impetus for releasing the package was to allow other people to experiment and report results. Â I’ve looked at a lot of graphs, but thereÂ is a limit to the number of books that I know well enough to be able to make an objective comparison between the SyuzhetÂ graph and my close reading of the book. This is another place where AnnieÂ raises some red flags. Â AnnieÂ calls attention to these two images (below) from my earlier post and complains that the transformed graph is not a good representation of the noisy raw data. Â She writes: The full trajectory opens with a largely flat stretch and a strong negative spike around x=1100 that then rises back to be neutral by about x=1500. The foundation shape, on the other hand, opens with aÂ rise, and in fact peaks in positivity right around where the original signal peaks in negativity. In other words, the foundation shape for the first part of the book is not merely inaccurate, but in fact exactly opposite the actual shape of the original graph. Annie’s reading of the graphs, though,Â is inconsistent with the overall plot of the novel, whereas the transformed plot is perfectly consistent with the novel. What Annie calls a “strong negative spike” is the scene in which Stephen is pandied by Father Arnell. Â It is an important negative moment, to be sure, but not nearly as important, or as negative, as the major dip that occurs midway through the novel–when Stephen experiences Hell.Â The scene with Arnell is a minor blip compared to the pages and pages of hell and the pages and pages of anguish Stephen experiences before his confession.   Annie is absolutely correct in noting that there is information loss, but wrong in arguing that the graph fails to represent the novel. Â The tool has done what it was designed to do: it successfully reveals the overall shape of the narrative. Â The first third of the novel and the last third of the novel are considerably more positive than the middle section. Â But this is not meant to say or imply that the beginning and end are without negative moments. It is perfectly reasonable to want to see more of the page to page, or scene by sceneÂ fluctuations in sentiment, and that can be easily achieved by using the percentage segmentation method or by altering the low-pass filter size. Â Changing the filter size to retain five components instead of three results in the graph below. Â This new graph captures that “strong negative spike” (not so “strong” compared to hell) and reveals more of the novel’s ups and downs. Â This graph also provides more detail aboutÂ the end of the novel where Stephen comes down off his bird-girl high and moves towardÂ a more sober perspective forÂ his future. Portrait with Five Components Of course, the other reason for releasing the code is so that I can get suggestions for improvements. Annie (and a few others) have already propelled me to tweak several functions. Â AnnieÂ found (and reported on her blog) some legitimate flaws in the openNLP sentence parser. When it comes to passages with dialog, the openNLP parser falls down on the job. I ran a few dialog tests (including Annie’s example) and was able to fix the great majority of the sentence parsing errors by simply stripping out the quotation marks in advance. Based on Annie’s feedback, I’ve added a “quote stripping” parameter to the get_sentences() function. It’s all freshly baked and updated on github. But finally, I want to comment on Annie’s suggestion that some texts use irony and dark humor for more extended periods than you [that’s me] suggest in that footnoteâan assumption that can be tested by comparing human-annotated texts with the Syuzhet package. I think that would be a greatÂ test, and I hope that Annie will consider working with me, or in parallel, to test it. Â If anyone has any human annotated novels, please send them my/our way! Things like irony, metaphor, and dark humor are the monsters under theÂ bed that keep me up atÂ night. Still, I would not have released this code without doing a little bit of testing:-). These monsters can indeed wreak a bit of havoc, but usually they are all shadow and no teeth. Take the colloquial expression “That’s some bad R code, man.” This sentenceÂ is supposed to mean the opposite, as in “That is a fine bit of R coding, sir.” Â This is a sentenceÂ the tool isÂ not likely to get right; but, then again, thisÂ sentenceÂ also messes up my young daughter, and it tends to confuse EnglishÂ language learners. I have yet to find any sustained examples of this sort of construction in typical prose fiction, and I have made a fairly careful study of the emotional outliers in my corpus. Satire, extended satire in particular, is probably a more serious monster. Â Still, I would argue that the sentimentÂ tools performs exactly as expected; theyÂ just don’t understand what they areÂ “reading” in the way that we do. Â Then again, and this is no fabrication,Â I have had some (as in too many) college students over the years whoÂ haven’t understood what they were reading andÂ thought that Swift was being serious about eating succulent little babies in his Modest Proposal (those kookyÂ Irish)! So, some human beings interpret the sentiment in Modest ProposalÂ exactly as the sentiment parser does, which is to say, literally! (Check out the special bonus material at the bottom of this post for a graph of Modest Proposal.) I’d love to have a tool that could detect satire, irony, dark humor and the like, but such a tool is still a good ways off. Â In the meantime, we can take comfort in incremental progress. SpecialÂ thanks to Annie Swafford for prompting a stimulating discussion. Â Here is all the code necessary to repeat the experiments discussed above. . .library(syuzhet) path_to_a_text_file <- system.file("extdata", "portrait.txt", package = "syuzhet") joyces_portrait <- get_text_as_string(path_to_a_text_file) poa_v <- get_sentences(joyces_portrait)  # Get the four sentiment vectors stanford_sent <- get_sentiment(poa_v, method="stanford", "/Applications/stanford-corenlp-full-2014-01-04") bing_sent <- get_sentiment(poa_v, method="bing") afinn_sent <- get_sentiment(poa_v, method="afinn") nrc_sent <- get_sentiment(poa_v, method="nrc")  ###################################################### # Plot them using percentage segmentation ###################################################### plot(   scale(get_percentage_values(stanford_sent, 10)),    type = "l",    main = "Joyce's Portrait Using All Four Methods\n and Percentage Based Segmentation",    xlab = "Narrative Time",    ylab = "Emotional Valence",   ylim = c(-3, 3) ) lines(   scale(get_percentage_values(bing_sent, 10)),   col = "red",    lwd = 2 ) lines(   scale(get_percentage_values(afinn_sent, 10)),   col = "blue",    lwd = 2 ) lines(   scale(get_percentage_values(nrc_sent, 10)),   col = "green",    lwd = 2 ) legend('topleft', c("Stanford", "Bing", "Afinn", "NRC"), lty=1, col=c('black', 'red', 'blue',' green'), bty='n', cex=.75)  ###################################################### # Transform the Sentiments ###################################################### stan_trans <- get_transformed_values(   stanford_sent,    low_pass_size = 3,    x_reverse_len = 100,   scale_vals = TRUE,   scale_range = FALSE ) bing_trans <- get_transformed_values(   bing_sent,    low_pass_size = 3,    x_reverse_len = 100,   scale_vals = TRUE,   scale_range = FALSE ) afinn_trans <- get_transformed_values(   afinn_sent,    low_pass_size = 3,    x_reverse_len = 100,   scale_vals = TRUE,   scale_range = FALSE )  nrc_trans <- get_transformed_values(   nrc_sent,    low_pass_size = 3,    x_reverse_len = 100,   scale_vals = TRUE,   scale_range = FALSE )  ###################################################### # Plot them all ###################################################### plot(   stan_trans,    type = "l",    main = "Joyce's Portrait Using All Four Methods",    xlab = "Narrative Time",    ylab = "Emotional Valence",   ylim = c(-2, 2) )  lines(   bing_trans,   col = "red",    lwd = 2 ) lines(   afinn_trans,   col = "blue",    lwd = 2 ) lines(   nrc_trans,   col = "green",    lwd = 2 ) legend('topleft', c("Stanford", "Bing", "Afinn", "NRC"), lty=1, col=c('black', 'red', 'blue',' green'), bty='n', cex=.75)   ###################################################### # Sentence Parsing Annie's Example ###################################################### annies_sentences_w_quotes <- '"Mrs. Rachael, I neednât inform you who were acquainted with the late Miss Barbaryâs affairs, that her means die with her and that this young lady, now her aunt is deadâ" "My aunt, sir!" "It is really of no use carrying on a deception when no object is to be gained by it," said Mr. Kenge smoothly, "Aunt in fact, though not in law."'  # Strip out the quotation marks annies_sentences_no_quotes <- gsub("\"", "", annies_sentences)  # With quotes, Not Very Good: s_v <- get_sentences(annies_sentences_w_quotes) s_v  # Without quotes, Better: s_v_nq <- get_sentences(annies_sentences_no_quotes) s_v_nq  ###################################################### # Some Sentence Comparisons ###################################################### # Test one test <- "He was not the sort of man that one would describe as especially handsome." stanford_sent <- get_sentiment(test, method="stanford", "/Applications/stanford-corenlp-full-2014-01-04") bing_sent <- get_sentiment(test, method="bing") nrc_sent <- get_sentiment(test, method="nrc") afinn_sent <- get_sentiment(test, method="afinn") stanford_sent; bing_sent; nrc_sent; afinn_sent  # test 2 test <- "She was not the sort of woman that one would describe as beautiful." stanford_sent <- get_sentiment(test, method="stanford", "/Applications/stanford-corenlp-full-2014-01-04") bing_sent <- get_sentiment(test, method="bing") nrc_sent <- get_sentiment(test, method="nrc") afinn_sent <- get_sentiment(test, method="afinn") stanford_sent; bing_sent; nrc_sent; afinn_sent  # test 3 test <- "That's some bad R code, man." stanford_sent <- get_sentiment(test, method="stanford", "/Applications/stanford-corenlp-full-2014-01-04") bing_sent <- get_sentiment(test, method="bing") nrc_sent <- get_sentiment(test, method="nrc") afinn_sent <- get_sentiment(test, method="afinn") stanford_sent; bing_sent; nrc_sent; afinn_sentSPECIAL BONUS MATERIAL Swift’s classic satire presents some sentiment challenges. Â There is disagreement between the Stanford method and the other three in segment four where the sentiments move in opposite directions.  FOOTNOTE [1] By the way, I’m not sure if Annie was suggesting that the Stanford parser was not working because she could not get it to work (the NAs) or because there was something wrong in the syuzhet package code. The code, as written, works just fine on the two machines I have available for testing. I’d appreciate hearing from others who are having problems; my implementation definitely qualifies as a first class hack.
