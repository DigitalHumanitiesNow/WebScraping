Humanitarian andÂ development organizations like the United Nations and the World BankÂ typicallyÂ carry out disaster damage and needs assessments following major disasters.Â The ultimate goal of these assessments is to measure the impact of disasters on the society, economy and environment of the affected country or region. This includes assessing the damage caused to building infrastructure, for example.Â These assessmentÂ surveys are generally carried out in personâthat is, on foot and/or by driving around an affected area. This is a very time-consuming process with very variable results in terms of data quality.Â Can 3D (Point Clouds) derived from very high resolution aerial imagery captured by UAVs accelerate and improve the post-disaster damage assessment process? Yes, but a number of challenges related to methods, data &Â software need to be overcome first. Solving these challenges will require pro-active cross-disciplinary collaboration.  The followingÂ three-tiered scale is often used to classify infrastructure damage: “1) Completely destroyed buildings or those beyond repair; 2) Partially destroyed buildings with a possibility of repair; and 3) Unaffected buildings or those with only minor damageÂ .Â By locating on a map all dwellings and buildings affected in accordance with the categories noted above, it is easy to visualize the areas hardest hit and thus requiring priority attention from authorities in producing more detailed studies and defining demolition and debris removal requirements” (UN Handbook).Â As one World Bank colleague confirmedÂ in a recent email, “From the engineering standpoint, there are many definitions of the damage scales, but from years of working with structural engineers, I think the consensus is now to use a three-tier scale â destroyed, heavily damaged, and others (non-visible damage).” That said, field-based surveysÂ of disaster damage typically overlookÂ damage caused toÂ roofs since on-the-ground surveyors are bound by the laws of gravity. Hence the importance of satellite imagery. At the same time, however, “The primaryÂ problem is the vertical perspective of [satellite imagery, which]Â largely limits the building information to the roofs. This roof information is well suited for the identification of extreme damage states, that isÂ completely destroyed structures or, to a lesser extent, undamaged buildings.Â However, damage is a complex 3-dimensional phenomenon,” which means that “important damage indicators expressed onÂ building facÌ§ades, such as cracks or inclined walls, are largely missed, preventing an effective assessment of intermediate damage states” (Fernandez GalarettaÂ et al. 2014).  This explains why “Oblique imagery [captured from UAVs] has been identified as more useful,Â though the multi-angle imagery also adds a new dimension of complexity” as we experienced first-hand during the World Bank’s UAV response to Cyclone Pam in Vanuatu (Ibid, 2014).Â Obtaining photogrammetric data for oblique images is particularly challenging.Â That is, identifying GPS coordinates for a given house pictured in an oblique photograph is virtually impossible to do automaticallyÂ with the vast majority of UAV cameras. (Only specialist cameras using gimbal mounted systems can reportedly infer photogrammetric data in oblique aerial imagery, but even then it isÂ unclear how accurate this inferred GPS data is). In any event, obliqueÂ data also “lead to challenges resulting from the multi-perspective nature of the data, such as how to create single damage scores when multiple facÌ§ades are imaged”Â (Ibid, 2014). To this end,Â my colleagueÂ Jorge Fernandez Galaretta and I are exploring the use of 3D (point clouds) to assess disaster damage. Multiple software solutions like Pix4D and PhotoScanÂ can already be used to construct detailedÂ point clouds from high-resolution 2D aerial imagery (nadir andÂ oblique). “These exceed standard LiDAR point clouds in terms of detail, especially at facÌ§ades, and provide a rich geometric environment that favors the identification of more subtle damage features, such as inclined walls, that otherwise would not be visible, and that in combination with detailed facÌ§ade and roof imagery have not been studied yet”Â (Ibid, 2014). Unlike oblique images, point clouds give surveyors a full 3D view of an urban area, allowing them to “fly through” and inspect each building up close and from all angles. One need no longer be physically onsite, nor limited to simply one faÃ§ade or a strictly field-based view to determine whether a given building is partially damaged. But what does partially damaged even mean when this kind of high resolution 3D data becomes available?Â Take thisÂ recent note fromÂ a Bank colleague with 15+ years of experience in disaster damage assessments:Â “In the [Bank’s] official Post-Disaster Needs Assessment, the classification used is to say that if a building is 40% damaged, it needs to be repaired.Â In my view this is too vague a description and not much help. When we say 40%, is it the volume of the building we are talking about or the structural components?”  In their recent study, Fernandez GalarettaÂ et al. used point cloudsÂ to generate per-building damage scores based onÂ a 5-tiered classification scale (D1-D5). They chose toÂ compute these damage scores based on the following features: “cracks, holes, intersection of cracks with load-carrying elements and dislocated tiles.” They also selected non-damage related features: “facÌ§ade, window, column and intact roof.” Their results suggest that the visual assessment of point clouds is very useful to identify the following disaster damage features:Â total collapse, collapsed roof, rubble piles, inclined facÌ§ades andÂ more subtle damage signatures that are difficult toÂ recognize in more traditional BDA [Building Damage Assessment] approaches. The authors were thus able to compute a per building damage score, taking into account both “the overall structure of the building,” and the “aggregated information collected from each of the faÃ§ades and roofs of the building to provide an individual per-building damage score.” Fernandez GalarettaÂ et al. also explore theÂ possibility ofÂ automating this damage assessment process based on point clouds.Â Their conclusion: “More research is needed to extract automatically damage features from point clouds, combine those with spectral and pattern indicators of damage, and to couple this with engineering understanding of the significance of connected or occluded damage indictors for the overall structural integrity of a building.” That said, the authors note that this approachÂ would “still suffer from the subjectivity that characterizes expert-based image analysis.” Hence my interest inÂ using crowdsourcingÂ to analyze point clouds for disaster damage. Naturally,Â crowdsourcing alone will notÂ eliminate subjectivity. In fact, having more people analyze point clouds may yield all kinds of disparate results. This is explains why a detailed andÂ customized imagery interpretation guide is necessary;Â like this one, which was just released by my colleagues at the Harvard Humanitarian Initiative (HHI). This also explains why crowdsourcing platforms require quality-control mechanisms.Â One easy techniqueÂ is triangulation: have ten different volunteers look atÂ each point cloud and tag features in said cloud that showÂ cracks, holes, intersection of cracks with load-carrying elements and dislocated tiles. Surely more eyes are better than two forÂ tasks that require a good eye for detail.  Next, identify which features have the most tagsâthis is the triangulation process. For example, if one area of a point cloudÂ is tagged as a “crack” by 8 or moreÂ volunteers, chances are there really is a crack there. One can then count the total number of distinct areas tagged as cracks by 8 or more volunteers across the point cloud to calculate the total number of cracks per faÃ§ade. Do the same with the other metrics (holes, dislocated titles, etc.), and you can computer a per building damage score based on overall consensus derived from hundreds of crowdsourced tags. Note that “tags’ canÂ also be lines or polygons; meaning that individual cracks could be traced by volunteers, thus providingÂ information on the approximate lengths/size of a crack. This variable could also be factored in the overall per-building damage score. In sum, crowdsourcing could potentially overcome some of the data quality issues that have already marked field-based damage assessment surveys. In addition, crowdsourcing could potentially speed up the data analysis since professional imagery and GIS analysts tend to already be hugelyÂ busy in the aftermath of major disasters. Adding more data to their plate won’t help anyone. Crowdsourcing the analysis of 3D point clouds may thus be our best bet. So why hasn’t this all been done yet? For several reasons. For one, creating very high-resolution point clouds requires more pictures and thus more UAV flights, which can be time consuming. Second, processing aerial imagery to construct point clouds can also take some time. Third, handling, sharing and hosting point cloudsÂ can be challenging given how large those files quicklyÂ get. Fourth, no software platform currently exists to crowdsource the annotation of point clouds as described above (particularly when it comes to theÂ automated quality control mechanisms that are necessary to ensure data quality). Fifth, we need more robust imagery interpretation guides. Sixth,Â groups like the UN and the World Bank are still largelyÂ thinking in 2D rather than 3D. And those few who are considering 3D tend to approach this from a data visualization angleÂ rather than using human and machine computing to analyze 3D data. Seventh, this area, point cloud analysis for 3D feature detection, is still a very new area of research. Many of the methodology questions that need answers have yet to be answered, which is why my team and I at QCRI are starting to explore this area from the perspective of computer vision and machine learning. The holy grail? Combining crowdsourcing with machine learning for real-time feature detection of disaster damage in 3D point clouds rendered in real-time via airborne UAVs surveying a disaster site.Â So what is it going to take to get there? Well, first of all, UAVs are becoming more sophisticated; they’re flying faster and for longer and will increasingly be working in swarms. (In addition, many of the new micro-UAVs come with a “follow me” function, which could enable the easy and rapid collection of aerial imagery during field assessments).Â So the first challenge described above is temporary as areÂ the second and third challenges since computer processing power is increasing, not decreasing, over time. This leaves us with the software challenge and imagery guides. I’m already collaborate with HHI on the latter. As for the former, I’ve spoken with a number of colleagues to explore possible software solutions to crowdsource the tagging of point clouds. One idea is simply to extend MicroMappers.Â Another is to add simple annotation features to PLAS.io and PointCloudVizÂ since these platforms are already designed to visualize and interact with point clouds. A third option is to use a 3D model platform like SketchFab, which already enables annotations. (Many thanks to colleague Matthew SchroyerÂ for pointing me to SketchFab last week). I’ve since had a long call with SketchFab and am excited by the prospects of using this platform forÂ simple point cloud annotation. In fact, Matthew already used SketcFab to annotate a 3D model ofÂ Durbar Square neighborhood in downtown Kathmandu post-earthquake. HeÂ found an aerial video of the area, took multiple screenshots of this video, created a point cloud from these and then generated a 3D model which he annotated within SketchFab. This model, pictured below, would have been much higher resolution if he had the original footage or 2D images. Click pictures to enlarge.     Here’s a short video with all the annotations in the 3D model:  And here’s the link to the “live” 3D model. And to drive home the point that this 3D model could be far higher resolution if the underlying imageryÂ had been directly accessible to Matthew, check out this other SketchFab model below, which you can also access in full here.    The SketchFab team has kindly given me a SketchFab account that allows up to 50 annotations per 3D model. So I’ll be uploading a number of point clouds from Vanuatu (post Cyclone Pam) and Nepal (post earthquakes) to explore the usability of SketchFab for crowdsourced disaster damage assessments. In the meantime, one couldÂ simply tag-and-number all major features in a point cloud, create a Google Form, and ask digital volunteers to rate the level of damage near each numbered tag. Not a perfect solution, but one that works. Ultimately, we’d need users to annotate point clouds by tracingÂ 3D polygons if we wanted a more easy way toÂ use the resulting data for automated machine learning purposes. In any event, if readers do have any suggestions on other software platforms, methodologies, studies worthÂ reading, etc., feel freeÂ to get in touch via the comments section below orÂ by email,Â thank you. In the meantime,Â many thanks to colleagues Jorge, Matthew, Ferda & Ji (QCRI), Salvador (PointCloudViz), Howard (PLAS.io) and Corentin (SketchFab) for the time they’ve kindly spent brainstorming the above issues with me. 
