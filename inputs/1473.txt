A translation of my 2012-03-05 post "Quality Control for Crowdsourced Transcription" which appeared in "Etat de lâart en matiÃ¨re de Crowdsourcing dans les bibliothÃ¨ques numÃ©riques" by Moirez, Moreaux, and Josse (2013), reproduced for Francophone readers:Â«Single-track methodsÂ»: le document ne fait lâobjet que dâune seule transcription (par un seul contributeur ou de faÃ§on collaborative ensemble sur le mÃªme document)  Â«Open-ended community revisonÂ»: (WikipÃ©dia) les utilisateurs peuvent continuer Ã  modifier le texte transcrit, sans limite dans le temps. Un historique des modifications permet de revenir Ã  la version prÃ©cÃ©dente et dâÃ©viter le vandalisme. Â«Fixed-term community revisionÂ» (Transcribe Bentham) : convient pour des projets dâÃ©dition plus traditionnels, dont lâobjectif est la publication dâune âversion finaleâ. Quand une transcription atteint un niveau acceptable, val idÃ©e par les experts, elle est close et publiÃ©e.  Â«Community-controlled revision workflowsÂ» (Wikisource) : la transcription est considÃ©rÃ©e comme une âversion finaleâ non plus par des experts, mais parce quâelle a traversÃ© un workflow collaboratif de correction/rÃ©vision/validation - Â«Transcriptions with "known-bad" insertions before proofreadingÂ» : dans une premiÃ¨re phase, les correcteurs sont invitÃ©s Ã  transcrire. Puis dâautres correcteurs rÃ©visent la transcription en la comparant au texte original; pour sâassurer que la seconde lecture est bien rÃ©alisÃ©e, des erreurs sont ajoutÃ©es dans le texte: si toutes les Â«fausses erreursÂ» sont corrigÃ©es, le systÃ¨me dÃ©duit que les Â«vraies erreursÂ» ont dÃ» Ãªtre corrigÃ©es aussi.  Â«Single-keying with expert reviewÂ» : lorsquâune transcription a Ã©tÃ© rÃ©alisÃ©e par un contributeur, elle est validÃ©e ou rejetÃ©e par un expert (soit un professionnel de lâinstitution Ã  lâorigine du projet, soit un contributeur sÃ©lectionnÃ©). Si la correction est rejetÃ©e, elle est soit Ã  nouveau soumise Ã  correction, soit corrigÃ©e par lâexpert et validÃ©e. Â«Multi-track methodsÂ»: ces mÃ©thodes conviennent particuliÃ¨rement Ã  des corrections portant sur des donnÃ©es structurÃ©es ou des micro-tÃ¢ches. La mÃªme image de dÃ©part est prÃ©sentÃ©e Ã  plusieurs contributeurs qui transcrivent chacun Ã  partir de zÃ©ro. GÃ©nÃ©ralement, les contributeurs ne savent pas sâils sont les premiers correcteurs ou si dâautres transcriptions ont dÃ©jÃ  Ã©tÃ© soumises. Puis les donnÃ©es ainsi collectÃ©es sont comparÃ©es automatiquement.  Â«Triple-keying with votingÂ» (Old Weather, ReCAPTCHA) : lâimage est prÃ©sentÃ©e Ã  3 contributeurs, la majoritÃ© lâemporte (au depart, Old Weather proposait lâimage Ã  10 contributeurs, mais ils se sont aperÃ§us que la pertinence Ã©tait sensiblement la mÃªme avec 3 quâavec 10 contributeurs) Â«Double-keying with expert reconciliationÂ»: la mÃªme donnÃ©e est prÃ©sentÃ©e Ã  deux contributeurs, et, sâils ne sont pas dâaccord entre eux, un expert tranche. Â«Double-keying with emergent community-expert reconciliationÂ» (FamilySearch Indexing): la method est presque similaire Ã  la prÃ©cÃ©dente, sauf que lâexpert qui tranche entre deux corrections divergentes est lui-mÃªme un contributeur, qui a Ã©tÃ© promu conciliateur grÃ¢ce Ã  lâanalyse automatique de ses contributions (volume,pertinence). Â«Double-keying with N-keyed run-off votesÂ»: si les deux contributeurs ne sont pas dâaccord, la correction est re-proposÃ©e Ã  un nouveau duo/trio dâusagers. 
