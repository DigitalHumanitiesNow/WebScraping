Point Clouds, or 3D models derived from high resolution aerial imagery, are in fact nothing new. Several software platforms already exist to reconstruct a series of 2D aerial images into fully fledged 3D-fly-through models. Check out these very neat examples from my colleagues at Pix4D and SenseFly:    What does a castle, Jesus and a mountainÂ have to do with humanitarian action? As noted in my previous blog post, there’s only so much disaster damage one can glean from nadir (that is, vertical) imageryÂ andÂ oblique imagery. Lets suppose thatÂ the nadir image below was taken by an orbiting satellite or flying UAV right after anÂ earthquake, for example. How can you possibly assess disaster damage from thisÂ oneÂ picture alone? Even if you hadÂ nadir imagery for these houses before the earthquake, your ability to assess structural damage would be limited.  This explains why we also captured oblique imagery for the World Bank’sÂ UAV response to Cyclone Pam in Vanuatu (more hereÂ on that humanitarian mission). ButÂ even with oblique photographs, you’re stuck with one fixed perspective. Who knows what these houses below look like from the other side; your UAV may have simply captured this side only. And even if you had pictures for all possible angles, you’d literally have 100’s of pictures to leaf through and make sense of.  What’s that famous quote by Henry Ford again? “If I had asked people what they wanted, they would have said faster horses.”Â We don’t need faster UAVs, we simply need to turnÂ what we already have into Point Clouds, which I’m indeed hoping to do with the aerial imagery from Vanuatu, by the way. The Point Cloud below was made only from single 2D aerial images.  It isn’t perfect, but we don’t need perfection in disaster response, we need good enough. So whenÂ we as humanitarian UAV teams go into the next post-disaster deployment and ask what humanitarians they need, they mayÂ say “faster horses” because they’re not (yet) familiar with what’s really possible with theÂ imagery processing solutions available today. That obviously doesn’t mean that we should ignore their information needs. It simply means we should seekÂ to expand their imaginations vis-a-visÂ the art of the possible with UAVs and aerial imagery. Here is a 3D model of a village in Vanuatu constructed using 2D aerial imagery:  Now, the title of my blog post does lead with the wordÂ crowdsourcing. Why?Â For several reasons. First, it takes some decent computing power (and time) to create these Point Clouds. But if the underlying 2D imagery isÂ made available to hundreds of Digital Humanitarians, we could use this distributed computing powerÂ to rapidlyÂ crowdsource the creation ofÂ 3D models. Second, each model can then be pushed to MicroMappersÂ for crowdsourcedÂ analysis. Why? Because having aÂ dozen eyes scrutinizing one Point Cloud is better than 2. Note that for quality control purposes, each Point Cloud wouldÂ be shown to 5 different Digital Humanitarian volunteers; we already do this with MicroMappers for tweets, pictures, videos, satellite imagesÂ and of course aerial images as well. Each digital volunteer would thenÂ trace areas in the Point Cloud whereÂ they spot damage. If the traces from the different volunteers match, then bingo, there’s likely damage at thoseÂ x, y and z coordinate. Here’s the idea:  We could easily use iPads to turn the process into a Virtual Reality experience for digital volunteers. In other words, you’d be able to move around and above the actual PointÂ CloudÂ byÂ simply changing the position of yourÂ iPad accordingly. This technology alreadyÂ exists and has for several years now.Â TracingÂ features in the 3D models that appear to be damagedÂ would be as simple as using your finger toÂ outline the damageÂ on yourÂ iPad. What about the inevitable challenge of Big Data? What if thousandsÂ of Point Clouds are generated during aÂ disaster? Sure, we could try to scale our crowd-sourcing efforts by recruiting moreÂ Digital Humanitarian volunteers, but wouldn’t thatÂ just be asking for a “faster horse”? Just like we’ve already done with MicroMappers for tweets and text messages, we would seek to combine crowdsourcing andÂ Artificial Intelligence to automatically detect features of interest in 3D models. This sounds to me like an excellent research project for a research institute engaged in advanced computing R&D. I would love to see the results of this applied research integrated directly within MicroMappers. This would allow us to integrate the results of social media analysis via MicroMappers (e.g, tweets, Instagram pictures, YouTube videos) directly with the results of satellite imagery analysis as well as 2D andÂ 3DÂ aerial imagery analysis generated via MicroMappers. Anyone interested in working on this? 
