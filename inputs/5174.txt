In Part I, I argued that unsuccessful replications should more constructively be seen as scientific beginnings rather than ends. AsÂ promised, in Part II I will more concretely demonstrate this by organizing all of the available replication information for Schnall et al.âs (2008) studies using an approach being developed atÂ CurateScience.org. CurateScience.org aims to accelerate the growth of cumulative knowledge by organizing information about replication results and allowing constructive comments by the community of scientists regarding the careful interpretation of replication results. Links to available data, syntax files, and experimental materials will also be organized. The web platform aims to be a one-stop shop to locate, add, and modify suchÂ information and also facilitateÂ constructive discussions and new scholarship of published research findings. (The kinds of heated debates currently happening regardingÂ Schnall et al.’s studies that makes science so exciting — well, minus the ad hominem attacks!) Below is a screenshot of the organized replication results for the Schnall et al. (2008) studies, including links to available data files, forest plot graph of the effect size confidence intervals, and aggregated list of relevant blog posts and tweets.  As can be seen, there are actually 4Â additional direct replications in addition toÂ Johnson et al.’s (2014) special issueÂ direct replications. As mentioned in Part I, two “successful” direct replications have been reported for Schnall et al.’s Study 1. However, as can readily be seen, these two studies were under-powered (@60%) to detect the original d=-.60 effect sizeÂ and both effect size CIs include zero. Consequently, itÂ would beÂ inappropriate to characterize these studies as “successful” (the < .05 p-values reported on PsychFileDrawer.org were one-tailed tests). That being said, these studies should not be ignored given theyÂ contribute additional evidence that should countÂ toward one’s overall evaluation of the evidence for the claim that cleanliness priming influences moral judgments. Unsuccessful replications should also be viewed asÂ beginnings given that virtually all replicators make their data publicly available for verification and re-analysis (one of Curate Science’s focus). Hence, any interested researcher can download the data and re-analyze it from a different theoretical perspective and potentially gain new insights into the discrepant results. Data availability also plays an important role in interpreting replication results, especially in the case the results have not been peer-reviewed. That is, one should put more weight into replication results whose conclusions can be verified via re-analysis than replication results that do not have available data. Organizing replication results in this situation makes it clear that virtually allÂ of the replication efforts have targeted Schnall et al.’s Study 1. Only one direct replication is so far available for Shnall et al.’s Study 2. Though this replication study used a much larger sample and was pre-registered (hence more weight should be given to its results), it is not the case that the final verdict has been spoken. Our confidence in Study 2’s original results should decrease to some extent (assuming the replication results can be reproduced from the raw data), however, more evidence would be needed to further decrease our confidence. And even in the event of subsequentÂ negative results from high-powered direct replications (for either of Schnall et al.’s studies), it would still be possible that cleanliness priming can influence moral judgments using more accurate instruments or using more advanced designs (e.g., highly-repeated within-person designs). CurateScience.orgÂ aims to facilitate constructive discussions and theoretical debates of these kinds to accelerate the growth of cumulative knowledge in psychology/neuroscience (and beyond). Unsuccessful replications are beginnings, not ends. 
