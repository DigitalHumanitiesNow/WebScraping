I’m preparing for a conversation with Matt Gold and Amanda Licastro about mining the CHNM syllabus finder MySQL dump, and rather than keeping my notes and questions to myself I wanted to share them in a blog post. Earlier this year I blogged about some experiments that I’ve done to profile and process the dataset using Hadoop. I didn’t have the dedicated time to push my ideas further during the spring semester, but hope to change that this summer.  What would you do with 1.4 million syllabi? And with so many records, how would you know where to start? One of the unexpected discoveries within a few days of releasing the dataset was that while the mysqldump references the URLs of syllabi, most records do not have saved copies of their HTML. Doug Knox pointed out that the cached HTML only exists for about 17,000 websites. If the current uncompressed data dump is 500MB, can you imagine how large a completed dataset would be? 35GB uncompressed?  There are two technical questions related to the dataset that I’m interested in addressing:  How can we use data in the mysqldump to retrieve archived and cached copies of course syllabi, in order to establish a complete dataset? What technical platforms, standards, resources, or sample corpuses are necessary to make this approachable to the digital humanities community? These are large questions that will take a lot of time to answer, require partnerships to address the needs of computing time and storage, and the expertise of a broad base of people. Consider this blog post a draft of some ideas, and the beginning of a conversation.  Completing the Dataset  Using data from the mysqldump, we have syllabus information including url, title, date_added, and chnm_visited. I assume that the difference between date_added and chnm_visited is that chnm_visited includes the date of the most-recently cached copy of a page. I may be possible to use the Internet Archive’s Wayback Machine to retrieve saved copies of those pages (if they exist). While this sounds great in concept, my initial look into this indicated that it may be difficult to write a scraper to do this. Recently, Ed Summers blogged about structured data the Wayback Machine can return and has given me new hope that we could use this structured data to locate archived copies of syllabi.  If this is possible, there are a variety of other questions like what methods should be used to retrieve this information and how, whether the Internet Archive is cool with this type of batch processing being done, and a likely need for a more robust datastore.  Making The Dataset Approachable  At the time the syllabus corpus was originally released and we didn’t initially realize that it had missing values, I expect that most of us only glanced at a few thousand lines on the command line (if that) and closed the file. What else could we do? It drives home the problem that I think many large datasets face; they lack transparency, the ability to be explored, and perhaps most-importantly the ability to be hacked on. What if the dataset was 10,000 syllabi instead of 1.4 million; would it make a difference on how easily it could be investigated? What if there were only 1,000 syllabi? The question of how to make the dataset more approachable is interesting, and may involve both reducing the size of the dataset and working with platforms that lower the barrier of entry for users.  This summer I’m interning with Common Crawl, a non-profit that is creating an open crawl of the web and making the data accessible for developers and researchers. Common Crawl has a corpus of billions of webpages, and shares a basic problem of lack of approachability to people who may be interested in analyzing the data. There’s no way to browse the data without doing it in batch, and given the learning curve to use this data one may question their investment of time in the first place. My hunch is that a basic summary and visualization of the data would be an effective hook to interest people in using the data.  I think a barrier to doing big data analysis is that the data lacks the same hackability that is a property of many successful digital humanities tools and platforms. Most data mining tools and methods are heavy, and a high level of technical knowledge to get started. For this dataset to be of use to a large community of DHers, I think we should be mindful of the patchwork approach that is taken when we hack on code using scripting languages, where coding becomes a method of investigating and understanding an idea or concept. The initial work I was doing with the corpus used MRJob, a Python module developed by Yelp for writing Hadoop streaming MapReduce jobs. MRJob does a great job simplifying the processing of data, and it would be possible to create an Amazon Machine Image for interested DHers to begin using it in the cloud. There may be a handful of these small steps that could be taken, making it much easier for people to use.  Next Steps  It’s clear that pushing this project forward would require a great investment of time, and the collaboration of a large number of people. This is definitely a project larger than an individual. I would love to hear feedback on the two technical questions that I posed, and learn who else may be interested in helping out.  If you’re interested in contributing, let me know in the comments. If anyone is interested, I will be at THATCamp CHNM this upcoming weekend and would love to discuss this in the hallways or at a session. 
